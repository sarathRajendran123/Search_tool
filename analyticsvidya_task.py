# -*- coding: utf-8 -*-
"""AnalyticsVidya_task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ck8P1A1lKRKEl7hSF4PhA_wi0mrcP9-x
"""

pip install transformers faiss-cpu numpy pandas

import requests
from bs4 import BeautifulSoup
import time

# Base URL for the course collection
base_url = "https://courses.analyticsvidhya.com/collections?page="

# Initialize an empty list to store course data
courses = []

# Loop through each page
page_num = 1
while True:
    # Fetch the HTML content of the current page
    url = base_url + str(page_num)
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    # print(soup)
    # Find all course cards on the page
    course_cards = soup.select('.products__list-item')
    if not course_cards:
        # If there are no course cards, break the loop (end of pagination)
        break

    # Extract data from each course card
    for card in course_cards:
        title = card.select_one("h3").text.strip()
        link = "https://courses.analyticsvidhya.com" + card.find("a", class_="course-card")["href"]

        # Visit each course page to get description and curriculum
        course_page = requests.get(link)
        course_soup = BeautifulSoup(course_page.content, "html.parser")
        # Get course description
        description = course_soup.select_one(".fr-view").get_text(strip=True) if course_soup.select_one(".fr-view") else "No description available"

        # Get course curriculum items
        curriculum_items = course_soup.select(".course-curriculum li")
        curriculum = [item.get_text(strip=True) for item in curriculum_items] if curriculum_items else ["No curriculum available"]

        # Store course data in a dictionary
        course_data = {
            "title": title,
            "description": description,
            "curriculum": curriculum,
            "link": link
        }
        courses.append(course_data)

        # Delay to be polite to the server
        time.sleep(1)  # Adjust based on the server's speed and your requirements
        print(course_data)
    # Increment page number for the next iteration
    page_num += 1

# Print out all courses data
for course in courses:
    print(course)

# Install necessary libraries
!pip install sentence-transformers faiss-cpu

from sentence_transformers import SentenceTransformer
import numpy as np

# Initialize the embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Extract course descriptions (assuming you have course data from your scraping)
course_descriptions = [course['description'] for course in courses]

# Generate embeddings for each course description
course_embeddings = model.encode(course_descriptions, convert_to_tensor=True)

import faiss

# Convert embeddings to numpy array (if needed)
embeddings_matrix = np.array([embedding.cpu().detach().numpy() for embedding in course_embeddings])

# Create the index
embedding_dim = embeddings_matrix.shape[1]  # Get the dimensionality of embeddings
index = faiss.IndexFlatL2(embedding_dim)  # L2 distance (Euclidean)
index.add(embeddings_matrix)  # Add embeddings to the index

# Optional: save the index to disk for later use
faiss.write_index(index, "course_index.faiss")

def search_courses(query, top_k=5):
    # Encode the query using the same model
    query_embedding = model.encode([query], convert_to_tensor=True)

    # Convert to numpy array and reshape to (1, embedding_dim) for FAISS
    query_embedding_np = np.array([query_embedding.cpu().detach().numpy()]).reshape(1, -1)

    # Perform the search in the FAISS index
    distances, indices = index.search(query_embedding_np, top_k)

    # Retrieve matching courses and distances
    results = []
    for idx, distance in zip(indices[0], distances[0]):
        course = courses[idx]  # Get the course details
        results.append({
            "title": course['title'],
            "description": course['description'],
            "link": course['link'],
        })
    return results

# Example search
query = "Learn about choosing the right LLM"
results = search_courses(query)

# Display results
for result in results:
    print(f"Title: {result['title']}")
    print(f"Description: {result['description']}")
    print(f"Link: {result['link']}")
    print("\n")

!pip install gradio

# Import necessary libraries
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import gradio as gr

# Initialize the embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Prepare data (Use your existing course data)
# Assuming `courses` list is already populated from your scraping step.

# Generate embeddings for each course description
course_descriptions = [course['description'] for course in courses]
course_embeddings = model.encode(course_descriptions, convert_to_tensor=True)

# Create the FAISS index for fast searching
embedding_dim = course_embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(np.array([embedding.cpu().detach().numpy() for embedding in course_embeddings]))

# Search function using FAISS
def search_courses(query, top_k=5):
    query_embedding = model.encode([query], convert_to_tensor=True)
    query_embedding_np = np.array([query_embedding.cpu().detach().numpy()]).reshape(1, -1)
    distances, indices = index.search(query_embedding_np, top_k)

    results = []
    for idx, distance in zip(indices[0], distances[0]):
        course = courses[idx]
        results.append({
            "title": course['title'],
            "description": course['description'],
            "link": course['link'],
        })
    return results

# Gradio interface
def gradio_search(query):
    results = search_courses(query)
    # Format results for display
    formatted_results = "\n\n".join([f"**Title**: {result['title']}\n"
                                     f"**Description**: {result['description']}\n"
                                     f"[Link]({result['link']})"
                                     for result in results])
    return formatted_results

# Gradio App
iface = gr.Interface(
    fn=gradio_search,
    inputs=gr.Textbox(lines=2, placeholder="Enter your search query here"),
    outputs="markdown",
    title="Smart Course Search",
    description="Search for courses using a query to find relevant course information."
)

# Launch Gradio app (comment out when deploying on Hugging Face Spaces)
iface.launch()

